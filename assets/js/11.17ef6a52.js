(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{414:function(a,t,e){"use strict";e.r(t);var s=e(4),n=Object(s.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("p",[e("img",{staticClass:"lazy",attrs:{alt:"image","data-src":"https://drive.google.com/uc?export=view&id=1Dh3QKqKygHROqVqjQ2BHf6P8mz--EXyc",loading:"lazy"}})]),a._v(" "),e("h2",{attrs:{id:"_1-batch-gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-batch-gradient-descent"}},[a._v("#")]),a._v(" "),e("strong",[a._v("1.Batch gradient descent")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-&space;\\eta&space;*&space;\\nabla&space;_{\\theta}&space;J(\\theta)"}}),a._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("nb_epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    params_grad "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" evaluate_gradient"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("loss_function"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" params"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("-")]),a._v(" learning_rate "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" params_grad\n")])])]),e("p",[a._v("Batch gradient descent is guaranteed to converge to the global minimum for convex error  surface and to a local minimum for non-convex surfaces.")]),a._v(" "),e("h2",{attrs:{id:"_2-stochastic-gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-stochastic-gradient-descent"}},[a._v("#")]),a._v(" "),e("strong",[a._v("2. Stochastic gradient descent")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-&space;\\eta&space;*&space;\\nabla&space;_{\\theta}&space;J(\\theta;&space;x^{i};y^{i})"}}),a._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("nb_epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    np"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("random"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("shuffle"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" example "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n        params_grad "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" evaluate_gradient"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("loss_function"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" example"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" params"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n        params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("-")]),a._v(" learning_rate"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" params_grad\n")])])]),e("ul",[e("li",[e("p",[a._v("SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily")])]),a._v(" "),e("li",[e("p",[a._v("SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively")])])]),a._v(" "),e("h2",{attrs:{id:"_3-mini-batch-gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-mini-batch-gradient-descent"}},[a._v("#")]),a._v(" "),e("strong",[a._v("3.Mini-batch gradient descent")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-&space;\\eta&space;*&space;\\nabla&space;_{\\theta}&space;J(\\theta;&space;x^{i:i&plus;n};y^{i:i&plus;n})"}}),a._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("nb_epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    np"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("random"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("shuffle"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" batch "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" get_batches"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" batch_size"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("50")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n        params_grad "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" evaluate_gradient"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("loss_function"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" batch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" params"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n        params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" params "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("-")]),a._v(" learning_rate "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" params_grad\n")])])]),e("ul",[e("li",[a._v("Common mini-batch sizes range between 50 and 256, but can vary for different applications.")]),a._v(" "),e("li",[a._v("Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used")])]),a._v(" "),e("h2",{attrs:{id:"_4-momentum-for-gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-momentum-for-gradient-descent"}},[a._v("#")]),a._v(" "),e("strong",[a._v("4. Momentum for Gradient descent.")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}v_{t}&space;=&space;\\gamma&space;v_{t-1}&space;&plus;&space;\\eta&space;*&space;\\nabla&space;_{\\theta}&space;J(\\theta)&space;"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-v_{t}"}}),a._v(" "),e("ul",[e("li",[a._v("The momentum term "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\gamma"}}),a._v("  is usually set to 0.9 or a similar value.")]),a._v(" "),e("li",[a._v("The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way")])]),a._v(" "),e("p",[a._v("Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector.")]),a._v(" "),e("h2",{attrs:{id:"_5-nesterov-accelerated-gradient"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-nesterov-accelerated-gradient"}},[a._v("#")]),a._v(" "),e("strong",[a._v("5. Nesterov accelerated gradient")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}v_{t}&space;=&space;\\gamma&space;v_{t-1}&space;&plus;&space;\\eta&space;*&space;\\nabla&space;_{\\theta}&space;J(\\theta-\\gamma&space;*&space;m)"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-v_{t}"}}),a._v(" "),e("ul",[e("li",[a._v("This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks")])]),a._v(" "),e("h2",{attrs:{id:"_6-adagrad"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_6-adagrad"}},[a._v("#")]),a._v(" "),e("strong",[a._v("6.Adagrad")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}s&space;=&space;s&space;&plus;&space;\\nabla&space;J(\\theta)&space;\\bigodot&space;\\nabla&space;J(\\theta)"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-\\frac{\\eta}{\\sqrt{s&plus;\\epsilon}}&space;\\bigodot&space;\\nabla&space;J(\\theta)"}}),a._v(" "),e("ul",[e("li",[e("p",[a._v("One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate.")])]),a._v(" "),e("li",[e("p",[a._v("Adagrad modifies the general learning rate "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\gamma"}}),a._v("  at each time step t for every parameter "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta_{i}"}}),a._v("  based on the past gradients that have been computed for "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta_{i}"}})])])]),a._v(" "),e("h2",{attrs:{id:"_7-rmsprop"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_7-rmsprop"}},[a._v("#")]),a._v(" "),e("strong",[a._v("7. RMSprop")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}v_{t}&space;=&space;\\gamma&space;v_{t-1}&space;&plus;&space;(1&space;-&space;\\gamma)&space;*&space;\\nabla&space;J(\\theta)&space;\\bigodot&space;\\nabla&space;J(\\theta)&space;"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-v_{t}"}}),a._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[a._v("tf"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("train"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("RMSPropOptimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("learning_rate"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("learning_rate"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" momentum"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("0.9")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" decay"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("0.9")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" epsilon"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("1e-10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),e("ul",[e("li",[a._v("RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\gamma"}}),a._v("  to be set to 0.9, while a good default value for the learning rate "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\eta"}}),a._v(" is 0.001.")])]),a._v(" "),e("h2",{attrs:{id:"_8-adaptive-moment-estimation-adam"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_8-adaptive-moment-estimation-adam"}},[a._v("#")]),a._v(" "),e("strong",[a._v("8. Adaptive moment estimation (Adam)")])]),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}m&space;=&space;\\beta_{1}&space;*&space;m&space;&plus;&space;(1-\\beta_{1})*\\nabla&space;J(\\theta)"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}s&space;=&space;\\beta_{2}&space;*&space;s&space;&plus;&space;(1-\\beta_{2})&space;*&space;\\nabla&space;J(\\theta)&space;\\bigodot&space;\\nabla&space;J(\\theta)"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}m&space;=&space;\\frac{m}{1-\\beta_{1}^{t}}"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}s&space;=&space;\\frac{s}{1-\\beta_{2}^{t}}"}}),a._v(" "),e("br"),a._v(" "),e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\theta&space;=&space;\\theta&space;-&space;\\frac{\\eta}{\\sqrt{s&plus;\\epsilon}}&space;\\bigodot&space;m"}}),a._v(" "),e("h3",{attrs:{id:"where"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#where"}},[a._v("#")]),a._v(" where:")]),a._v(" "),e("ul",[e("li",[e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\beta_{1}"}}),a._v(" = 0.9")]),a._v(" "),e("li",[e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\beta_{2}"}}),a._v(" = 0.99999")]),a._v(" "),e("li",[e("img",{attrs:{src:"https://latex.codecogs.com/svg.image?\\inline&space;\\LARGE&space;\\bg{white}\\epsilon"}}),a._v(" = 10^-8")])]),a._v(" "),e("h2",{attrs:{id:"resference"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#resference"}},[a._v("#")]),a._v(" "),e("strong",[a._v("Resference:")])]),a._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/",target:"_blank",rel:"noopener noreferrer"}},[a._v("https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);t.default=n.exports}}]);