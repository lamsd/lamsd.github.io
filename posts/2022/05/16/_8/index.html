<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>An overview of gradient descent optimization algorithms | Theme</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.ebb6bbac.css" as="style"><link rel="preload" href="/assets/js/app.a8204b6a.js" as="script"><link rel="preload" href="/assets/js/11.0cfbcc4d.js" as="script"><link rel="prefetch" href="/assets/js/10.1a829d6d.js"><link rel="prefetch" href="/assets/js/12.e68c5b7c.js"><link rel="prefetch" href="/assets/js/2.3a20468b.js"><link rel="prefetch" href="/assets/js/3.7806952e.js"><link rel="prefetch" href="/assets/js/4.00554625.js"><link rel="prefetch" href="/assets/js/5.7104daac.js"><link rel="prefetch" href="/assets/js/6.0d966f3a.js"><link rel="prefetch" href="/assets/js/7.fa5c8586.js"><link rel="prefetch" href="/assets/js/8.d0c8c071.js"><link rel="prefetch" href="/assets/js/9.24bc8167.js">
    <link rel="stylesheet" href="/assets/css/0.styles.ebb6bbac.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><section id="global-layout" data-v-4fb7124e><header class="header" data-v-61b62cbe data-v-4fb7124e><div class="header-navbar" data-v-61b62cbe><div class="flex-xbc main header-nav" data-v-61b62cbe><div class="nav-link" data-v-61b62cbe><a href="/" class="inblock link-logo router-link-active" data-v-61b62cbe><img data-src="/logo.png" loading="lazy" alt="logo" class="logo-img lazy" data-v-61b62cbe></a> <nav class="link-list" data-v-61b62cbe><a href="/" class="list-item router-link-active" data-v-61b62cbe>Home</a><a href="/posts/" class="list-item router-link-active" data-v-61b62cbe>Posts</a><a href="/tag/" class="list-item" data-v-61b62cbe>Tags</a><a href="/category/" class="list-item" data-v-61b62cbe>Categories</a><a href="/about/" class="list-item" data-v-61b62cbe>About</a></nav></div> <div class="search-box" data-v-61b62cbe><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div></div></div> </header> <!----> <section class="page" data-v-4fb7124e data-v-4fb7124e><section class="info no-bg" data-v-52fe94f0><article class="main info-content" data-v-a5c9dc12 data-v-52fe94f0><div class="content-header" data-v-a5c9dc12><h1 class="header-title" data-v-a5c9dc12>An overview of gradient descent optimization algorithms</h1></div> <div class="flex-wcc content-tag" data-v-a5c9dc12><div class="inblock tag-list" data-v-a5c9dc12><a href="/category/ML/" class="tag-text" data-v-a5c9dc12>ML
      </a></div> <span class="tag-space" data-v-a5c9dc12>/</span> <div class="inblock tag-list" data-v-a5c9dc12><a href="/tag/Display/" class="tag-text" data-v-a5c9dc12>Display
      </a></div></div> <div class="content content__default" data-v-a5c9dc12><p><img alt="image" data-src="https://drive.google.com/uc?export=view&amp;id=1Dh3QKqKygHROqVqjQ2BHf6P8mz--EXyc" loading="lazy" class="lazy"></p> <h2 id="_1-batch-gradient-descent"><a href="#_1-batch-gradient-descent" class="header-anchor">#</a> <strong>1.Batch gradient descent</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\eta&amp;space;*&amp;space;\nabla&amp;space;_{\theta}&amp;space;J(\theta)"> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span> data<span class="token punctuation">,</span> params<span class="token punctuation">)</span>
    params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate <span class="token operator">*</span> params_grad
</code></pre></div><p>Batch gradient descent is guaranteed to converge to the global minimum for convex error  surface and to a local minimum for non-convex surfaces.</p> <h2 id="_2-stochastic-gradient-descent"><a href="#_2-stochastic-gradient-descent" class="header-anchor">#</a> <strong>2. Stochastic gradient descent</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\eta&amp;space;*&amp;space;\nabla&amp;space;_{\theta}&amp;space;J(\theta;&amp;space;x^{i};y^{i})"> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    <span class="token keyword">for</span> example <span class="token keyword">in</span> data<span class="token punctuation">:</span>
        params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span> example<span class="token punctuation">,</span> params<span class="token punctuation">)</span>
        params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate<span class="token operator">*</span> params_grad
</code></pre></div><ul><li><p>SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily</p></li> <li><p>SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively</p></li></ul> <h2 id="_3-mini-batch-gradient-descent"><a href="#_3-mini-batch-gradient-descent" class="header-anchor">#</a> <strong>3.Mini-batch gradient descent</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\eta&amp;space;*&amp;space;\nabla&amp;space;_{\theta}&amp;space;J(\theta;&amp;space;x^{i:i&amp;plus;n};y^{i:i&amp;plus;n})"> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> get_batches<span class="token punctuation">(</span>data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> params<span class="token punctuation">)</span>
        params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate <span class="token operator">*</span> params_grad
</code></pre></div><ul><li>Common mini-batch sizes range between 50 and 256, but can vary for different applications.</li> <li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li></ul> <h2 id="_4-momentum-for-gradient-descent"><a href="#_4-momentum-for-gradient-descent" class="header-anchor">#</a> <strong>4. Momentum for Gradient descent.</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}v_{t}&amp;space;=&amp;space;\gamma&amp;space;v_{t-1}&amp;space;&amp;plus;&amp;space;\eta&amp;space;*&amp;space;\nabla&amp;space;_{\theta}&amp;space;J(\theta)&amp;space;"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-v_{t}"> <ul><li>The momentum term <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\gamma">  is usually set to 0.9 or a similar value.</li> <li>The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way</li></ul> <p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector.</p> <h2 id="_5-nesterov-accelerated-gradient"><a href="#_5-nesterov-accelerated-gradient" class="header-anchor">#</a> <strong>5. Nesterov accelerated gradient</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}v_{t}&amp;space;=&amp;space;\gamma&amp;space;v_{t-1}&amp;space;&amp;plus;&amp;space;\eta&amp;space;*&amp;space;\nabla&amp;space;_{\theta}&amp;space;J(\theta-\gamma&amp;space;*&amp;space;m)"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-v_{t}"> <ul><li>This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks</li></ul> <h2 id="_6-adagrad"><a href="#_6-adagrad" class="header-anchor">#</a> <strong>6.Adagrad</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}s&amp;space;=&amp;space;s&amp;space;&amp;plus;&amp;space;\nabla&amp;space;J(\theta)&amp;space;\bigodot&amp;space;\nabla&amp;space;J(\theta)"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-\frac{\eta}{\sqrt{s&amp;plus;\epsilon}}&amp;space;\bigodot&amp;space;\nabla&amp;space;J(\theta)"> <ul><li><p>One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate.</p></li> <li><p>Adagrad modifies the general learning rate <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\gamma">  at each time step t for every parameter <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\theta_{i}">  based on the past gradients that have been computed for <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\theta_{i}"></p></li></ul> <h2 id="_7-rmsprop"><a href="#_7-rmsprop" class="header-anchor">#</a> <strong>7. RMSprop</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}v_{t}&amp;space;=&amp;space;\gamma&amp;space;v_{t-1}&amp;space;&amp;plus;&amp;space;(1&amp;space;-&amp;space;\gamma)&amp;space;*&amp;space;\nabla&amp;space;J(\theta)&amp;space;\bigodot&amp;space;\nabla&amp;space;J(\theta)&amp;space;"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-v_{t}"> <div class="language-python extra-class"><pre class="language-python"><code>tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>RMSPropOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> decay<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e-10</span><span class="token punctuation">)</span>
</code></pre></div><ul><li>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\gamma">  to be set to 0.9, while a good default value for the learning rate <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\eta"> is 0.001.</li></ul> <h2 id="_8-adaptive-moment-estimation-adam"><a href="#_8-adaptive-moment-estimation-adam" class="header-anchor">#</a> <strong>8. Adaptive moment estimation (Adam)</strong></h2> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}m&amp;space;=&amp;space;\beta_{1}&amp;space;*&amp;space;m&amp;space;&amp;plus;&amp;space;(1-\beta_{1})*\nabla&amp;space;J(\theta)"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}s&amp;space;=&amp;space;\beta_{2}&amp;space;*&amp;space;s&amp;space;&amp;plus;&amp;space;(1-\beta_{2})&amp;space;*&amp;space;\nabla&amp;space;J(\theta)&amp;space;\bigodot&amp;space;\nabla&amp;space;J(\theta)"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}m&amp;space;=&amp;space;\frac{m}{1-\beta_{1}^{t}}"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}s&amp;space;=&amp;space;\frac{s}{1-\beta_{2}^{t}}"> <br> <img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{150}\bg{white}\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\frac{\eta}{\sqrt{s&amp;plus;\epsilon}}&amp;space;\bigodot&amp;space;m"> <h3 id="where"><a href="#where" class="header-anchor">#</a> where:</h3> <ul><li><img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\beta_{1}"> = 0.9</li> <li><img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\beta_{2}"> = 0.99999</li> <li><img src="https://latex.codecogs.com/gif.image?\inline&amp;space;\LARGE&amp;space;\dpi{100}\bg{white}\epsilon"> = 10^-8</li></ul> <h2 id="resference"><a href="#resference" class="header-anchor">#</a> <strong>Resference:</strong></h2> <ol><li><a href="https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/" target="_blank" rel="noopener noreferrer">https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol></div> <div class="content-time" data-v-a5c9dc12><time datetime="5/16/2022, 7:00:00 AM" class="time-text" data-v-a5c9dc12>Create Time: 5/16/2022, 7:00:00 AM
    </time> <time datetime="5/16/2022, 4:48:29 PM" class="time-text" data-v-a5c9dc12>Last Updated: 5/16/2022, 4:48:29 PM
    </time></div></article> <section class="flex-xb main info-nav" data-v-e08c9474 data-v-52fe94f0><!----> <a href="/posts/2022/04/14/_7/" class="flex-xb nav-item" data-v-e08c9474><div class="flex-xcc item-img" data-v-e08c9474><img data-src="https://picsum.photos/id/1021/2048/1206" loading="lazy" alt="Using Rasa" class="img lazy" data-v-e08c9474></div> <article class="flex-ysc item-content" data-v-e08c9474><h2 class="content-title" data-v-e08c9474>Using Rasa</h2> <div class="content" data-v-e08c9474></div></article></a></section> <!----></section></section> <footer class="footer" data-v-5bc4f524 data-v-4fb7124e><nav class="link-list" data-v-5bc4f524><a href="https://github.com/tolking" target="_blank" rel="noopener noreferrer" class="list-item" data-v-5bc4f524>Github</a></nav> <a href="/" class="copyright router-link-active" data-v-5bc4f524>Theme © 2022</a></footer></section><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.a8204b6a.js" defer></script><script src="/assets/js/11.0cfbcc4d.js" defer></script>
  </body>
</html>
